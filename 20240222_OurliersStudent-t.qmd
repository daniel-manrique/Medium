---
title: "Untitled"
editor: visual
---

# Do not overthink about 'outliers', use a student-t distribution instead

For many researchers, outliers are rogue waves that can dramatically alter the course of the analysis or "confound" some expected effects. I prefer to use the term "extreme observations" and leave the concept of outlier for observations that do not really belong to the population being studied. For example, in my field (brain ischemia research), an outlier will be an animal that does not have ischemia (when it should have), while extreme observations will be animals with small or large ischemias that differ greatly from the others.

Traditional statistical models are built on the strong foundation of Gaussian distributions. This has a significant limitation: an inherent assumption that all data points will cluster around a central mean in a predictable pattern (based on the central limit theorem). This may be true in Plato's world of ideas, but we, scientists in the biomedical field, are well aware that it is extremely difficult to rely on this assumption given the limited sampling (number of animals) we have available to make observations.

Gaussian distributions are very sensitive to extreme observations, and using them makes scientists think that eliminating extreme observations is the best option to get more "clear" or "clean" results (whatever that means). As I once commented in an article being reviewer 2, "The issue is not the extreme observations that may"hide" your effects, it is the fact that you are using a statistical model that (I believe) is inappropriate for your purposes". It should be noted that no statistical model is the "right" or "appropriate" one, but we can estimate that, given the data, there are certain statistical models that are more likely to generate the observed data (generative models) than others.

Fortunately, nothing forces us to be bound by the assumptions of the Gaussian models, right? We have other options, such as the **Student's t-distribution**. I see it as a more adaptable vessel to navigate the turbulent seas of real-world biomedical data. The Student's t-distribution provides a robust alternative to acknowledge that our data may be populated by extreme observations that are normal biological responses that we can expect in any context. There may be patients or animals that don't respond or overreact to a treatment, and it is valuable that our modeling approach recognizes these responses as part of the spectrum. Therefore, this tutorial explores the modeling strategies using Student's t-distributions through the lens of the **`brms`** package for R---a powerful ally for Bayesian modeling.

# What's begind a student's t-distribution?

A Student's t-distribution is nothing more than a Gaussian distribution with heavier tails. In other words, we can say that the Gaussian distribution is a special case of the Student's t-distribution. The Gaussian distribution is defined by the mean (μ) and the standard deviation (σ). The Student t distribution, on the other hand, adds an additional parameter, the degrees of freedom (df), which controls the "thickness" of the distribution. This parameter assigns more probability to events further from the mean. This feature is particularly useful for small sample sizes, such as in biomedicine, where the assumption of normality is questionable. Note that as the degrees of freedom increase, the Student t-distribution approaches the Gaussian distribution. We can observe this in the following:

```{r}
# Load necessary libraries
library(ggplot2)

# Set seed for reproducibility
set.seed(123)

# Define the distributions
x <- seq(-4, 4, length.out = 200)
y_gaussian <- dnorm(x)
y_t3 <- dt(x, df = 3)
y_t10 <- dt(x, df = 10)
y_t30 <- dt(x, df = 30)

# Create a data frame for plotting
df <- data.frame(x, y_gaussian, y_t3, y_t10, y_t30)

# Plot the distributions
ggplot(df, aes(x)) +
  geom_line(aes(y = y_gaussian, color = "Gaussian")) +
  geom_line(aes(y = y_t3, color = "t, df=3")) +
  geom_line(aes(y = y_t10, color = "t, df=10")) +
  geom_line(aes(y = y_t30, color = "t, df=30")) +
  labs(title = "Comparison of Gaussian and Student t-Distributions",
       x = "Value",
       y = "Density") +
  scale_color_manual(values = c("Gaussian" = "blue", "t, df=3" = "red", "t, df=10" = "green", "t, df=30" = "purple")) +
  theme_classic()

```
Note that the hill around the mean gets smaller as the degrees of freedom decrease, because this probability mass goes to the tails, which are thicker. This facet decreases the student's t-distribution sensitivity to outlines.  



The simulated dataset contains columns for the group (Control or Treatment), the animal ID, the the measurement instance, and the time spent on the rotarod.

```{r}
set.seed(8807) # 

# Parameters
n_animals <- 7 # Number of animals per group
measurements_per_animal <- 3 # Number of measurements per animal
groups <- c("Control", "Treatment") # Groups

# Simulate data
data <- expand.grid(Group = groups,
                    AnimalID = 1:n_animals,
                    Measurement = 1:measurements_per_animal)

base_time_control <- runif(n_animals, min = 195, max = 195)  
base_time_treatment <- runif(n_animals, min = 250, max = 250)  

# Map base times to each animal in data
data$BaseTime <- ifelse(data$Group == "Control", 
                        rep(base_time_control, each = measurements_per_animal),
                        rep(base_time_treatment, each = measurements_per_animal))

# Add random noise to each measurement to increase within-animal variability
data$TimeSpent <- data$BaseTime + rnorm(nrow(data), mean = 0, sd = 30)  

# Ensure TimeSpent values are within the original intended ranges
data$TimeSpent <- pmax(pmin(data$TimeSpent, 300), 150)

# Adjust AnimalID to include group name for uniqueness across groups
data$AnimalID <- with(data, paste(Group, AnimalID, sep = "_"))

head(data)

```

Now, we generate a dataset to summarizing the measurements by calculating the mean

```{r}
# Calculate the average TimeSpent for each AnimalID across measurements
averaged_data <- data %>%
  group_by(Group, AnimalID) %>%
  summarise(AverageTimeSpent = mean(TimeSpent), .groups = 'drop')

# View the first few rows of the averaged data
head(averaged_data)

```

With the simulations done, we can start with the analysis.

# Exploratory data visualization (EDV)

Before we begin modeling, we load the required packages. If you haven't already installed these packages, please do so.

```{r}
library(ggplot2)
library(brms)
library(patchwork)
library(tidybayes)
library(ggplot2)
library(ggdist)
library(modelr)
library(dplyr)

```

Let's generate box plots to see what our data looks like:

```{r}
set.seed(8807) 

# Plot for the averaged data set
avergared_edv <- ggplot(averaged_data, 
                        aes (x = Group,
                             y = AverageTimeSpent)) +
    geom_boxplot() +
    geom_jitter(width = 0.2) +
    scale_y_continuous(limits = c(150, 300)) +
    theme_classic() +
    theme(text = element_text(size = 30))

# Plot for the complete data set
complete_edv <- ggplot(data, 
                        aes (x = Group,
                             y = TimeSpent)) +
    geom_boxplot() +
    geom_jitter(aes(color = AnimalID), width = 0.2) +
    theme_classic() +
    scale_y_continuous(limits = c(150,300)) +
    theme(legend.position = "none") +
    theme(text = element_text(size = 30))
    

avergared_edv | complete_edv 
```

We can see that the tendency in both graphs is the same, the control group with a lower mean than the treated. However, we see that the graph with all the points has a higher uncertainty. Note that we are not respecting the hierarchy of the data here. Our task, of course, is to do better than that.

# Fit the avaraged model

First, we fit a traditional model using the mean of the three measurements. For this exercise, we will use the default flat priors from \`brms'. In a real modeling context, it may be wiser to choose better priors.

```{r}
# Define the Bayesian model with the average model
Average_Fit <- brm(AverageTimeSpent ~ Group, 
           data = averaged_data, 
           family = gaussian(),
           seed = 8807,
           control = list(adapt_delta = 0.99),
           # this is to save the model in my laptop
           file    = "Models/20240203_BeyondTheMean/Average_Fit.rds",
           file_refit = "never")

summary(Average_Fit)
```

In this case, let's just skip the model diagnostics and go straight to the results. But never do this in scientific research. Always validate that your model's predictions are consistent with your understanding of the world, and that your model could possibly have generated the data.

The 'intercept' (192 ± 7) represents the average time spent (AverageTimeSpent) for the baseline group, which is the control group in this context. This group spent approximately 194 seconds on the rotarod. The 95% uncertainty interval ranges from 178 to 206 seconds, indicating considerable precision in this estimate.

On the other hand, the 'GroupTreatment' coefficient (52 ± 10) indicates the difference in the average time spent between the treatment group and the control group. Treated animals spent on average 52 seconds more than those in the control group. The uncertainty intervals (31 to 72) indicate that it is very likely that this contrast is positive. Is it meaningful? That's something scientists should decide using scientific knowledge. Despite the current dogma of "significance" by which scientists operate, there is no coefficient that interprets data or answers scientific questions for you. That's our job as scientists. My experience with the rotarod tells me that a difference of almost a minute is quite large, relevant.

I do not want to overlook the `sigma` parameter, which represents the residual standard deviation of the time spent on the rotarod, capturing the variability in the observations that is not explained by the time points or group membership. The estimate indicates that there is a small amount of scatter in the data (18 ± 3), with a confidence interval ranging from 12 to 27. Note that this unexplained variance is less than the estimated effect.

Now let's plot the results of the model. We will use the \`tidybayes' package to do this. First, let's plot the full posterior distributions:

```{r}
set.seed(8807)

averaged_data %>%
  data_grid(Group) %>%
  add_predicted_draws(Average_Fit) %>%
  ggplot(aes(x = Group, y = .prediction, fill = Group)) +
  stat_halfeye() +
  geom_jitter(data = averaged_data, 
             aes(x = Group, y = AverageTimeSpent),
             width = 0.2,
             size = 2)+
  labs(title = "Posterior distribution",
       y = "Time on the rod",
       x = "Group") +
  theme_classic() +
  theme(legend.position = "top") +
  theme(text = element_text(size = 30)) +
  coord_flip()
```

In addition to interpreting the coefficients, the graphical results show the full posterior distributions as \`stat_halfye' and raw data points. Remember that we are dealing with a data set where we have averaged three measurements at each time point for the animals. It may be more informative to look directly at the contrast between the two groups:

```{r}
set.seed(8807)

# Extracting posterior samples
posterior_samples <- posterior_samples(Average_Fit, c("b_Intercept", "b_GroupTreatment"))

# Visualizing the GroupTreatment effect
Average_Fig <- ggplot(posterior_samples, aes(x = b_GroupTreatment)) +
  stat_halfeye() +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 1) +
  labs(title = "Posterior the effect size",
       y = "Density",
       x = "Effect Size (Treatment - Control)") +
  scale_x_continuous(breaks = seq(0, 100, 25)) +
  theme_classic() +
  theme(text = element_text(size = 30)) 

Average_Fig
```

The full posterior for the contrast between the two groups enable us to make more informed scientific inferences. From this angle, this model suggest that the treatment is having a prominent effect on the time the rodents spend in the rotarod.

Now, let's go to the second phase, where we respect the hierarchy of the data and make use of all the information we have.

# Hierarchical modeling

The core of our modeling strategy is to incorporate the variability inherent in the repeated measurements at each time point. In this way, we aim to model the underlying variability directly, rather than simplifying the data by averaging. The importance of this approach is that we respect the hierarchy of the data. In our case, measurements are nested within animals.

```{r}
Complete_Fit <- brm(TimeSpent ~ Group + (1 | AnimalID), 
           data = data, 
           family = gaussian(),
           chains = 4, 
           iter = 5000, 
           warmup = 2500,
           control = list(adapt_delta = 0.99),
           # this is to save the model in my laptop
           file    = "Models/20240203_BeyondTheMean/Complete_Fi1.rds",
           file_refit = "never")

summary(Complete_Fit)
```

The interpretation of the main effects is similar to that of our average_data model. Here we can focus on the additional coefficient for `AnimalID`. With a value of 8 ± 5, it shows the standard deviation of the animal-specific intercepts around the overall mean. This tells us how much each animal varies from the average of all animals. I judge that a variation of 8 is not particularly meaningful for the time in the rotarod. Nevertheless, we incorporate this knowledge into our model.

As before, we generate full posterior distributions, this time specifying `add_predicted_draws(re_formula = NA)` to include all random effects:

```{r}
set.seed(8807)

predicted_draws <- data %>%
  group_by(Group, AnimalID) %>% # Ensure we consider the animal-level grouping
  add_predicted_draws(Complete_Fit, re_formula = NA) # re_formula = NA to include all random effects

# Plotting
ggplot(predicted_draws, aes(x = Group, y = .prediction, fill = Group)) +
  stat_halfeye() + # Visualize the posterior predictive distribution
  geom_point(data = predicted_draws %>% group_by(Group, AnimalID) %>% summarise(TimeSpent = mean(.prediction)), 
             aes(x = Group, y = TimeSpent, color = Group), 
             shape = 1, size = 3, show.legend = FALSE) + # Mean prediction per animal
  geom_jitter(data = data, aes(x = Group, y = TimeSpent, color = Group),
              width = 0.15, alpha = 0.6, size = 2) + # Actual measurements
  labs(title = "Posterior Distribution",
       y = "Time on the Rod",
       x = "Group") +
  theme_classic() +
  theme(legend.position = "top") +
  theme(text = element_text(size = 30)) +
  coord_flip()
```

The estimates here are virtually identical to those of our first model. In fact, they are indistinguishable to the naked eye. We can see the same thing when we plot the contrast together with that of our first model:

```{r}
# Extract posterior samples for the treatment effect
posterior_samples <- posterior_samples(Complete_Fit, pars = c("b_GroupTreatment"))

# Convert to a data frame for ggplot
posterior_df <- as.data.frame(posterior_samples)

# Visualizing the Treatment effect size with uncertainty
Complete_Fig <- ggplot(posterior_df, aes(x = b_GroupTreatment)) +
  stat_halfeye() +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 1) +
  labs(title = "Posterior for the effect size",
       x = "Effect Size (Treatment - Control)",
       y = "Density") +
  scale_x_continuous(breaks = seq(0, 100, 25)) +
  theme_classic() +
  theme(text = element_text(size = 30))

Average_Fig | Complete_Fig 
```

To visualize the group-level effects, we can consider two strategies: 1) plotting the full posterior for the group-level effects by specifying `pars = c("^sd_")` in the `posterior_samples` function:

```{r}
# Extract posterior samples for the random effect
posterior_samples <- posterior_samples(Complete_Fit, pars = c("^sd_"))

# Convert to a data frame for ggplot
posterior_df <- as.data.frame(posterior_samples)

# Visualizing the Treatment effect size with uncertainty
AnimalID_Fig <- ggplot(posterior_df, aes(x = sd_AnimalID__Intercept)) +
  stat_halfeye() +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 1) +
  labs(title = "Posterior for animal variability",
       x = "Animal (sd)",
       y = "Density") +
  scale_x_continuous(breaks = seq(0, 50, 10)) +
  theme_classic() +
  theme(text = element_text(size = 30))

AnimalID_Fig
```

This reveals a hidden aspect in our \`averaged_data model'. By considering the hierarchy of the data, we reveal the variability per animal, which in this case is no greater than the effect of treatment (given the simulation parameters we used). In this way, we provide information that allows us to make more informed decisions. While the groups differ by an average of 52 seconds, the same animals can vary up to about 20 seconds (if we look at the probability density).

Now let us examine the plot for individual animals:

```{r}

# Extracting group-level effects
posterior_samples <- Complete_Fit %>% 
  spread_draws(r_AnimalID[AnimalID,])  # Extract random effects for AnimalID

# Plotting the group-level effects
ggplot(posterior_samples, aes(x = r_AnimalID, y = AnimalID)) +
  stat_halfeye(fill = "dodgerblue", color = "black") +
  labs(title = "Group-Level Effects (AnimalID)",
       x = "Effect Size",
       y = "AnimalID") +
  theme_classic() +
  theme(text = element_text(size = 30))

```

So, what can we say at the end of this example? Well, for this simulation we have nearly identical intercepts (192 vs. 192), indicating that the average time spent for the control group is consistent across models. Keep in mind that this is not always the case. Here, the results is constrained by our simulation parameters. Second, we have a very close effect for treatment (51.99 vs. 52.12), meaning that the treatment group spends about 52 seconds more on the rod than the control group. In the context of the rotarod test, this is a meaningful difference.

Note that in our `averaged_data1` model, we did not account for the nested structure of our data (repeated measures within animals). We treat the average per animal. On the contrary, in our multilevel model, we accounted for the nested structure of the data by including random intercepts for each AnimalID, thus recognizing that measurements within the same animal are more similar. A question may arise here: given the similar intercepts and effects for treatment, can we simply continue with the first model? I believe that accounting for the true structure of the data is fundamental to accurate modeling, regardless of the similarity of the results or how the effects move in one direction or another. If we have information, we can use it fully in our models.

Now, our multilevel model estimated the standard deviation of the intercepts across animals (8) with an uncertainty interval from 0.30 to 21. This represents differences in baseline TimeSpent across animals that are not explained by the treatment effect. In fact, the increased residual variability in the multilevel model (31) indicates greater residual variability in TimeSpent after accounting for the fixed effects and random intercepts.In other words, we discovered that each animal has intrinsic variation that is not related to treatment.

You can find an updated version of this post on my GitHub site (https://daniel-manrique.github.io/). Let me know if this journey has been useful to you, and if you have any constructive comments to add to this exercise.

```{r}
sessionInfo()
```


