---
title-block-banner: true
title: "Avoiding abuse and misuse of T-test and ANOVA: Regression for categorical responses"
subtitle: "An aproach using Bayesian regression with brms"
date: today
date-format: full
author: 
  - name: "Daniel Manrique-Castano"
    orcid: 0000-0002-1912-1764
    affiliation: Univerisity Laval (Laboratory of neurovascular interactions) 
keywords: 
  - Student's-t distribution
  - Bayesian modeling
  - brms 
   
license: "CC BY"

format:
   pdf: 
    toc: true
    number-sections: true
    colorlinks: true
   html:
    code-fold: true
    embed-resources: true
    toc: true
    toc-depth: 2
    toc-location: left
    number-sections: true
    theme: spacelab

knitr:
  opts_chunk: 
    warning: false
    message: false
    
csl: science.csl
bibliography: References.bib
---

In neuroscience and other biomedical sciences, it is common to use behavioral tests to assess responses to experimental conditions or treatments. We can asses many aspects from basic motor and exploratory behaviors to memory and learning. Many of these variables are continuous (numerical) responses, i.e., they can take (finite or infinite) values in a given range. The time in the open field, the weight of the animals or the number of cells in a brain region can be considered, in general terms, as continuous numerical variables.

There are, however, other types of variables we researchers record in our experiments. A very common one are ordered categorical variables, also called ordinal variables. These are categorical variables that have a natural order, analogous to the well known surveys we answer whether we agree or disagree with a statement, being 0 totally disagree and 5 totally agree. To facilitate the registration of this variables in printed or digital datasheets, we codify them (by convention) as numbers. Such is the case of the 5-point Bederson score used in the context of cerebral ischemia research in rodent models [@bieber2019], which is coded as follows:

-   0 = no observable deficit

-   1 = forelimb flexion

-   2 = forelimb flexion and decreased resistance to lateral push

-   3 = circling

-   4 = circling and spinning around the cranial-caudal axis

-   5 = no spontaneous movement.

Note that the numbers are only easy conventions. We could also use a,b,c,d,e; excellent, good, not so good, bad, very bad, almost dead; etc. I truly think it is not presumptuous to emphasize the self-evident nature of the matter . Surprisingly, however, @fig-sample [@onufriev2021; @liu2022] represents a particular and widespread malpractice that scientists in this field have been carrying out for years: They use t-tests and ANOVAS to analyze such ordered categorical variables.

![Left: Neurological score by Onufriev et. al (2021) (CC-BY). Right: Neurological score by Liu et al. (2022) (CC-BY).](Plots/2024-04-03_UseAndAbuseANOVA/Test%20example.png){#fig-sample fig-align="center"}

I still cannot find a logical explanation why dozens of authors, reviewers and editors feel comfortable with this scenario. When I have played the role of reviewer 2 and emphasize this point to the authors, asking them to explain why they evaluate a categorical response with a statistical test designed to deal with continuous numerical variables, what I get is a long list of published articles that perform this irrational practice. So, after all, I have found an answer to why are they conformable with this: what Gerd Gigerenzer (2004) calls "the ritual of mindless statistics" [@gigerenzer2004]. Indeed, most of us, scientists, have little idea of what we are doing with our data and simply repeat common malpractices passed down generation to generation.

In this article, we will then look at a more viable alternative for analyzing ordered categorical variables using `R`, `brms` [@brms-2]and elements from the `tidyverse` [@tidyverse].


# Facing the ritual of mindless statistics

To avoid the ritual of mindless statistics, we'll recreate the dataset from Liu et. al (2022) by eyeballing the data points and organize them in a table:

```{r}
#| label: CrateData
#| include: true
#| warning: false
#| message: false
#| column: margin

# We Define the observations
observations <- list(
  Sham = c(rep(0, 7)),
  tMCAO = c(rep(2, 3), rep(3, 2), rep(4, 2)),
  tMCAO_C = c(rep(1, 3), rep(2, 3), rep(3, 1))
)

# We create an empty data frame to populate
df <- data.frame(Group = character(), Response = integer())

# We populate the data frame
for (group in names(observations)) {
  for (response in unique(observations[[group]])) {
    df <- rbind(df, data.frame(Group = rep(group, sum(observations[[group]] == response)), 
                               Response = rep(response, sum(observations[[group]] == response))))
  }
}

head(df)
```
If at this point you look to the R code, you will realize that the `Response` variable is identified as a number with a range from 0 to 4. Off course, we are completely aware that this response is not numeric, but and ordered (ordinal) categorical variable. So, we explicitly make the conversion:

```{r}
#| label: ConvertData
#| include: true
#| warning: false
#| message: false
#| column: margin

df$Response <- factor(df$Response, levels = c("0", "1", "2", "3", "4"), ordered = TRUE)

str(df)

```
Now, we can verify is recognized as an ordered categorical variable. With this in hand we easily proceed to visualization and modeling.


## Exploratory data visualization

Let's first load the required libraries and create a visual theme for our plots.

```{r}
#| label: LoadPack
#| include: true
#| warning: false
#| message: false

library(ggplot2)
library(brms)
library(ggdist)
library(easystats)
library(dplyr)
library(tibble)

Plot_theme <- theme_classic() +
  theme(
      plot.title = element_text(size=18, hjust = 0.5, face="bold"),
      plot.subtitle = element_text(size = 10, color = "black"),
      plot.caption = element_text(size = 12, color = "black"),
      axis.line = element_line(colour = "black", size = 1.5, linetype = "solid"),
      axis.ticks.length=unit(7,"pt"),
     
      axis.title.x = element_text(colour = "black", size = 16),
      axis.text.x = element_text(colour = "black", size = 16, angle = 0, hjust = 0.5),
      axis.ticks.x = element_line(colour = "black", size = 1),
      
      axis.title.y = element_text(colour = "black", size = 16),
      axis.text.y = element_text(colour = "black", size = 16),
      axis.ticks.y = element_line(colour = "black", size = 1),
      
      legend.position="right",
      legend.direction="vertical",
      legend.title = element_text(colour="black", face="bold", size=12),
      legend.text = element_text(colour="black", size=10),
      
      plot.margin = margin(t = 10,  # Top margin
                             r = 2,  # Right margin
                             b = 10,  # Bottom margin
                             l = 10) # Left margin
      ) 
```

One straight forward manner to visualize categorical data is by means of a bar plot.

```{r}
#| label: fig-Fig2
#| include: true
#| warning: false
#| message: false
#| fig-cap: Response colored by group.
#| fig-height: 5
#| fig-width: 6

ggplot(df, aes(x = factor(Response), fill = Group)) +
  geom_bar(position = "dodge") +
  labs(x = "Response", y = "Count", title = "Response by Group") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1") +
  Plot_theme +
theme(legend.position = "top", legend.direction = "horizontal")
```

@fig-Fig2 shows the frequency per group. This is more in line with the logic of a categorical variable, rather than box plots which are more relevant for continuous numerical variables. Now, let's run a regression to dissentangle the misteryes (if any) of this dataset.

# Fitting statistical models with brms

Here we fit our model with `Day` and `Genotype` as interacting categorical predictors for the distribution of `Trial 3`. Let's first fit a typical Gaussian model, which is analogous to an ordinary least squares (OLS) model from the frequentist framework, since we are using the default flat `brms` [priors](https://paul-buerkner.github.io/brms/reference/set_prior.html). Priors are beyond the scope of this article, but I promise we'll cover them in a future blog.

Once we have results from the Gaussian model, we can compare them to the large results from the Student's t model. We then add`sigma` to the equation to account for the difference in the variance of the data.

## Fitting a "typical" (frequentists) model in Gaussian land

Our Gaussian model is built under the typical (and often incorrect) assumption of homoscedasticity [@yang2019]. In other words, we assume that all groups have the same (or very similar) variance. I do not recall seeing this as a researcher.

```{r}
#| label: GaussianFit
#| include: true
#| warning: false
#| message: false
#| results: false

Gaussian_Fit1 <- brm(Trial3 ~ Day * Genotype, 
           data = data, 
           family = gaussian(),
           # seed for reproducibility purposes
           seed = 8807,
           control = list(adapt_delta = 0.99),
           # this is to save the model in my laptop
           file    = "Models/20240222_OutliersStudent-t/Gaussian_Fit1.rds",
           file_refit = "never")

# Add loo for model comparison
Gaussian_Fit1 <- 
  add_criterion(Gaussian_Fit1, c("loo", "waic", "bayes_R2"))
```

### Model diagnostics

Before proceeding, it's a good idea to do some simple model diagnostics to compare the actual observations with the predictions made by our model. We can do this in several ways, but the most common is to plot full densities. We can achieve this using the `pp_check` function from `brms`.

```{r}
#| label: fig-GaussianDiag1
#| include: true
#| warning: false
#| message: false
#| fig-cap: Diagnostics for the Gaussian model
#| fig-height: 4
#| fig-width: 5

set.seed(8807)

pp_check(Gaussian_Fit1, ndraws = 100) +
  labs(title = "Gaussian model") +
  theme_classic()
```

@fig-GaussianDiag1 suggests that our observations (dark blue) are not meaningfully different from the model predictions. Below, I leave you with additional code to check other `pp_check` alternatives with their respective graphs.

```{r}
#| label: fig-GaussianDiag2
#| include: true
#| warning: false
#| message: false
#| fig-height: 5
#| fig-width: 5
#| column: screen-inset-shaded
#| layout-nrow: 1

set.seed(88071)


pp_check(Gaussian_Fit1, group = "Genotype", type = "dens_overlay_grouped", ndraws = 100) +
  labs(title = "Density by Genotype") +
    theme_classic()

pp_check(Gaussian_Fit1, type = "stat_grouped", group = "Genotype", stat = "var", binwidth = 3) +
  coord_cartesian(xlim = c(0, 300)) +
  ggtitle("Grouped variance") +
  theme_classic()

pp_check(Gaussian_Fit1, type = "stat", stat = "var", binwidth = 3) +
  coord_cartesian(xlim = c(0, 600)) +
  ggtitle("How well we captured the variace") +
  theme_classic()

pp_check(Gaussian_Fit1, type = "stat", stat = "mean", binwidth = 2) +
  coord_cartesian(xlim = c(0, 50)) +
  ggtitle("How well we captured the mean") +
  theme_classic()
```

### Checking the results for the Gaussian distribution

Now, we use the `describe_posterior` function from the `bayestestR` package [@bayestestR] to see the results:

```{r}
#| label: Gaussian_Posterior
#| include: true
#| warning: false
#| message: false

describe_posterior(Gaussian_Fit1,
                   centrality = "mean",
                   dispersion = TRUE,
                   ci_method = "HDI",
                   test = "rope",
                   )
```

Let's focus here on the 'intercept', which is the value for WT at 1 DPI, and 'GenotypeKO', the estimated difference for KO animals at the same time point. We see that WT animals spend about 37 seconds in the rotarod, while their KO counterparts spend less than a second (0.54) more. As a researcher in this field, I can say that this difference is meaningless and that genotype has no effect on rotarod performance. Even the effect of day, which is 2.9, seems meaningless to me under this model. We can easily visualize these estimates using the wonderful `conditional_effects` function from brms.

```{r}
#| label: fig-GaussianEff
#| include: true
#| warning: false
#| message: false
#| fig-cap: Conditional effects for the Gaussian model
#| fig-height: 5
#| fig-width: 7

# We create the graph for convex hull
Gaussian_CondEffects <- 
  conditional_effects(Gaussian_Fit1)

Gaussian_CondEffects <- plot(Gaussian_CondEffects, 
       plot = FALSE)[[3]]

Gaussian_CondEffects + 
  geom_point(data=data, aes(x = Day, y = Trial3, color = Genotype), inherit.aes=FALSE) +
  Plot_theme +
  theme(legend.position = "bottom", legend.direction = "horizontal")
```

In @fig-GaussianEff we can see the estimates and uncertainty for the interaction terms. I have customized the plot with a number of ggplot elements, which you can check in the original [Quarto Notebook](https://github.com/daniel-manrique/MediumBlog/blob/main/20240222_OutliersStudent-t.qmd). Note the similar uncertainty for both time points, even though the dispersion is larger on day 1 than on day 2. We will address this point in a small snippet at the end of this article.

Now let's see how much our understanding changes when we model the same data using a student-t distribution.

## Fitting our guest: a model with a student-t distribution

It's time to use the student-t distribution in our `brms` model.

```{r}
#| label: StudentFit
#| include: true
#| warning: false
#| message: false
#| results: false

Student_Fit <- brm(Trial3 ~ Day * Genotype, 
           data = data, 
           family = student,
           # seed for reproducibility purposes
           seed = 8807,
           control = list(adapt_delta = 0.99),
           # this is to save the model in my laptop
           file    = "Models/20240222_OutliersStudent-t/Student_Fit.rds",
           file_refit = "never")

# Add loo for model comparison
Student_Fit <- 
  add_criterion(Student_Fit, c("loo", "waic", "bayes_R2"))
```

### Model diagnostics

We plot the model diagnostics as done before:

```{r}
#| label: fig-StudentDiag1
#| include: true
#| warning: false
#| message: false
#| fig-cap: Model diagnostics for student-t distribution
#| fig-height: 4
#| fig-width: 5

set.seed(8807)

pp_check(Student_Fit, ndraws = 100) +
  labs(title = "Student-t model") +
  theme_classic()
```

@fig-StudentDiag1 shows that the mean shape and the peak of the observations and the predictions match. It's important to note that our model seems to predict values below 0. This is an important research issue that we will skip for now. However, it does imply the use of informative priors or distribution families that set a lower bound at 0, such as the `log_normal',`hurdle_lognormal', or \`zero_inflated_poisson', depending on the case. Andrew Heiss [@heiss2021] offers a [great example](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/) in this regard.

### Checking the results for the student-t distribution

Let's take a look at the posterior distribution:

```{r}
#| label: Student_Posterior
#| include: true
#| warning: false
#| message: false

describe_posterior(Student_Fit,
                   centrality = "mean",
                   dispersion = TRUE,
                   ci_method = "HDI",
                   test = "rope",
                   )
```

Under this model, we can see that our estimates have changed moderately, I would say. Our estimate for the intercept (WT at 1 day) is reduced by 7 seconds. And why is that? Because the extreme values we discovered at the beginning have less influence on the measures of central tendency of the data. Thus, this is a more accurate measure of the "typical" WT animal on day 1. We also observe a substantial increase in the effect of day, with almost 10 seconds more than our initial Gaussian estimates. Importantly, the effect of our KO genotype appears to be more notorious, increasing about 10 times from 0.52 in our Gaussian model to 5.5 in our student-t model. From my perspective, given the context of these data, the contrasts between the two models are notorious.

Let's see it in graphical terms using `conditional_effects`:

```{r}
#| label: fig-StudentEff
#| include: true
#| warning: false
#| message: false
#| fig-cap: Conditional effects for the Student-t model
#| fig-height: 5
#| fig-width: 7

Student_CondEffects <- 
  conditional_effects(Student_Fit)

Student_CondEffects <- plot(Student_CondEffects, 
       plot = FALSE)[[3]]

Student_CondEffects + 
  geom_point(data=data, aes(x = Day, y = Trial3, color = Genotype), inherit.aes=FALSE) +
  Plot_theme +
  theme(legend.position = "bottom", legend.direction = "horizontal")
```

Can we get better estimates? For this particular example, I think we can. From the start, it was easy to notice the difference in the variance of the data, especially when we compare the first and second-day visuals. We improved our estimates using the student-t distribution, and we can improve them further by developing a model for heteroscedasticity that predicts sigma (the residual variance). In this way, the model does not assume that your residual variance is equal across your grouping variables, but it becomes a response that can be modeled by predictors. This is the little point we left for the end.

## Predicting sigma using a student-t distribution

We include sigma as a response variable using the`bf` function from `brms`. In this case, we are going to model this parameter using the same predictors `Day` and `Genotype`.

```{r}
#| label: StudentFit?Sigma
#| include: true
#| warning: false
#| message: false
#| results: false

Student_Mdl2 <- bf (Trial3 ~ Day * Genotype,
                     sigma ~ Day * Genotype)

Student_Fit2 <- brm(
           formula = Student_Mdl2,
           data = data, 
           family = student,
           # seed for reproducibility purposes
           seed = 8807,
           control = list(adapt_delta = 0.99),
           # this is to save the model in my laptop
           file    = "Models/20240222_OutliersStudent-t/Student_Fit2.rds",
           file_refit = "never")

# Add loo for model comparison
Student_Fit2 <- 
  add_criterion(Student_Fit2, c("loo", "waic", "bayes_R2"))
```

### Model diagnostics

```{r}
#| label: fig-StudentDiag2
#| include: true
#| warning: false
#| message: false
#| fig-cap: Model diagnostics for student-t distribution with sigma
#| fig-height: 4
#| fig-width: 5

set.seed(8807)

pp_check(Student_Fit2, ndraws = 100) +
  labs(title = "Student-t") +
  theme_classic()
```

@fig-StudentDiag2 looks good, except for the uncomfortable predictions below 0. For this case, I judge that this does not strongly bias the estimates and their uncertainty. However, this is an aspect I will take into account when doing actual research.

### Checking the results for the student-t distribution with predicted sigma

Now, let's take a look at the posterior distribution.

```{r}
#| label: Student_Posterior2
#| include: true
#| warning: false
#| message: false

describe_posterior(Student_Fit2,
                   centrality = "mean",
                   dispersion = TRUE,
                   ci_method = "HDI",
                   test = "rope",
                   )
```

We see more parameters compared to the other two fitted models because the response for sigma is now included as a main effect in the model. Under this scheme, we see that the intercepts are closer to those of the Gaussian model and the effect of genotype (`GenotypeKO`) is reduced by half.

There is one aspect to note, however. In our first Student-t model, the uncertainty for the intercept was 24.1--37.4. On the other hand, in the last model, the uncertainty increases to 24.3--46.1. This means that when we consider the different variances, we are less certain of this (and other) parameters. The same is true for day, for example, which changes from 1.2--18.9 to -5.6--18.1. In this case, we are now less certain that the second day is associated with an increase in time spent on the rotarod. Don't worry, the purpose of statistical modeling is to provide the best possible quantification of the uncertainty in a measurement, and that's what we're doing right now. Of course, our uncertainty increases when we have extreme values that are part of our sample and therefore part of our population. In this example, we see that accounting for the different variances in our data gives us a very different idea of our results.

Finally, we can see that sigma, plotted on the log scale, varies meaningfully with day and genotype:

```{r}
#| label: fig-StudentCondEffects2
#| include: true
#| warning: false
#| message: false
#| fig-cap: Conditional effects for the Student-t model with sigma
#| fig-height: 5
#| fig-width: 5
#| layout-nrow: 1

Student_CondEffects2 <- 
  conditional_effects(Student_Fit2)

Student_CondEffects2 <- plot(Student_CondEffects2, 
       plot = FALSE)[[3]]

Student_CondEffects2 + 
  geom_point(data=data, aes(x = Day, y = Trial3, color = Genotype), inherit.aes=FALSE) +
  Plot_theme +
  theme(legend.position = "bottom", legend.direction = "horizontal")


Student_CondEffects3 <- 
  conditional_effects(Student_Fit2, dpar = "sigma")

Student_CondEffects3 <- plot(Student_CondEffects3, 
       plot = FALSE)[[3]]

Student_CondEffects3 + 
  Plot_theme +
  theme(legend.position = "bottom", legend.direction = "horizontal")
```

What we see in the second graph is sigma, which effectively accounts for the variance in this parameter between days and genotypes. We see a much higher uncertainty at day 1, especially for WT mice, while the parameter is analogous at day 2.

We can conclude this article by comparing the three models for out-of-sample predictions.

## Model comparison

We perform model comparison using the WAIC criteria [@gelman2013]for estimating the out-of-sample prediction error. By considering both the log-likelihood of the observed data and the effective number of parameters, it provides a balance between model fit and complexity. Unlike some other criteria, WAIC inherently accounts for the posterior distribution of the parameters rather than relying on point estimates, making it particularly suited to Bayesian analyses.

Given a data set and a Bayesian model, the WAIC is calculated as:

$$
\text{WAIC} = -2 \times \left( \text{LLPD} - p_{\text{WAIC}} \right)
$$

Where: $\text{LLPD}$ is the log pointwise predictive density, calculated as the average log likelihood for each observed data point across the posterior samples. $\text{WAIC}$ is the effective number of parameters, computed as the difference between the average of the log likelihoods and the log likelihood of the averages across posterior samples.

We use the `compare_performance` function from the `performance` package, part of the `easystats` environment [@performance; @makowski2019; @bayestestR].

```{r}
#| label: Models_Compare
#| include: true
#| warning: false
#| message: false
#| results: false

Fit_Comp <- 
  compare_performance(
    Gaussian_Fit1, 
    Student_Fit, 
    Student_Fit2, 
    metrics = "all")

Fit_Comp
```

The output shows that our Student-t model predicting sigma is the least penalized (WAIC = 497) for out-of-sample prediction. Note that there is no estimate for sigma in this model because it was included as a response variable. This table also shows that the student-t model has less residual variance (sigma) than the Gaussian model, which means that the variance is better explained by the predictors. We can visualize the same results as a graph:

```{r}
#| label: fig-Models_Graph
#| include: true
#| warning: false
#| message: false
#| results: false
#| fig-cap: Model comparison by WAIC
#| fig-height: 5
#| fig-width: 8

Fit_Comp_W <- 
loo_compare(
 Gaussian_Fit1, 
    Student_Fit, 
    Student_Fit2,  
  criterion = "waic")

# Generate WAIC graph
Fit_Comp_WAIC <- 
  Fit_Comp_W[, 7:8] %>% 
  data.frame() %>% 
  rownames_to_column(var = "model_name") %>% 
  
ggplot(
  aes(x    = model_name, 
      y    = waic, 
      ymin = waic - se_waic, 
      ymax = waic + se_waic)
  ) +
  geom_pointrange(shape = 21) +
  scale_x_discrete(
    breaks=c("Gaussian_Fit1", 
             "Student_Fit", 
             "Student_Fit2"), 
            
    labels=c("Gaussian_Fit1", 
             "Student_Fit", 
             "Student_Fit2") 
             
    ) +
  coord_flip() +
  labs(x = "", 
       y = "WAIC (score)",
       title = "") +
  Plot_theme

Fit_Comp_WAIC
```

@fig-Models_Graph shows that our last model is less penalized for out-of-sample prediction.

You can find an updated version of this post on my [GitHub site](https://github.com/daniel-manrique/MediumBlog/blob/main/20240222_OutliersStudent-t.qmd). Let me know if this journey was useful to you, and if you have any constructive comments to add to this exercise.

# References

::: {#refs}
:::

```{r}
sessionInfo()
```
