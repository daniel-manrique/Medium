---
title-block-banner: true
title: "Avoiding abuse and misuse of T-test and ANOVA: Regression for categorical responses"
subtitle: "An aproach using Bayesian regression with brms"
date: today
date-format: full
author: 
  - name: "Daniel Manrique-Castano"
    orcid: 0000-0002-1912-1764
    degrees:
      - PhD
    affiliation: 
      - name: Univerisity Laval 
        department: Psychiatry and Neuroscience
        group: Laboratory of neurovascular interactions 
note: "GitHub: https://daniel-manrique.github.io/"
keywords: 
  - Ordinal regression distribution
  - Bayesian modeling
  - brms 
   
license: "CC BY"

format:
   pdf: 
    toc: true
    number-sections: true
    colorlinks: true
   html:
    code-fold: true
    embed-resources: true
    toc: true
    toc-depth: 2
    toc-location: left
    number-sections: true
    theme: spacelab

knitr:
  opts_chunk: 
    warning: false
    message: false
    
csl: science.csl
bibliography: References.bib
---

In neuroscience and other biomedical sciences, it is common to use behavioral tests to assess responses to experimental conditions or treatments. We can asses many aspects from basic motor and exploratory behaviors to memory and learning. Many of these variables are continuous (numerical) responses, i.e., they can take (finite or infinite) values in a given range. The time in the open field, the weight of the animals or the number of cells in a brain region can be considered, in general terms, as continuous numerical variables.

There are, however, other types of variables we researchers record in our experiments. A very common one are ordered categorical variables, also called ordinal variables. These are categorical variables that have a natural order, analogous to the well known surveys we answer whether we agree or disagree with a statement, being 0 totally disagree and 5 totally agree. To facilitate the registration of this variables in printed or digital datasheets, we codify them (by convention) as numbers. Such is the case of the 5-point Bederson score used in the context of cerebral ischemia research in rodent models [@bieber2019], which is coded as follows:

-   0 = no observable deficit

-   1 = forelimb flexion

-   2 = forelimb flexion and decreased resistance to lateral push

-   3 = circling

-   4 = circling and spinning around the cranial-caudal axis

-   5 = no spontaneous movement.

Note that the numbers are only easy conventions. We could also use a,b,c,d,e; excellent, good, not so good, bad, very bad, almost dead; etc. I truly think it is not presumptuous to emphasize the self-evident nature of the matter . Surprisingly, however, [@fig-sample; @onufriev2021; @liu2022] represents a particular and widespread malpractice that scientists in this field have been carrying out for years: They use t-tests and ANOVAS to analyze such ordered categorical variables.

![Left: Neurological score by Onufriev et. al (2021) (CC-BY). Right: Neurological score by Liu et al. (2022) (CC-BY).](Plots/2024-04-03_UseAndAbuseANOVA/Test%20example.png){#fig-sample fig-align="center"}

I still cannot find a logical explanation why dozens of authors, reviewers and editors feel comfortable with this scenario. This is not only logical but more importantly, it leads to distorted effect sizes, low detection rates, and type-I-error rates, among others [@bürkner2019].

When I have played the role of reviewer 2 and emphasize this point to the authors, asking them to explain why they evaluate a categorical response with a statistical test designed to deal with continuous numerical variables, what I get is a long list of published articles that perform this irrational practice. So, after all, I have found an answer to why are they conformable with this: what Gerd Gigerenzer (2004) calls "the ritual of mindless statistics" [@gigerenzer2004]. Indeed, most of us, scientists, have little idea of what we are doing with our data and simply repeat common malpractices passed down generation to generation.

In this article, we will then look at a more viable alternative for analyzing ordered categorical variables using `R`, `brms` [@brms-2]and elements from the `tidyverse` [@tidyverse].

# Facing the ritual of mindless statistics

To avoid the ritual of mindless statistics, we'll recreate the dataset from Liu et. al (2022) by eyeballing the data points and organize them in a table:

```{r}
#| label: CrateData
#| include: true
#| warning: false
#| message: false
#| column: margin

# We Define the observations
observations <- list(
  Sham = c(rep(0, 7)),
  tMCAO = c(rep(2, 3), rep(3, 2), rep(4, 2)),
  tMCAO_C = c(rep(1, 3), rep(2, 3), rep(3, 1))
)

# We create an empty data frame to populate
df <- data.frame(Group = character(), Response = integer())

# We populate the data frame
for (group in names(observations)) {
  for (response in unique(observations[[group]])) {
    df <- rbind(df, data.frame(Group = rep(group, sum(observations[[group]] == response)), 
                               Response = rep(response, sum(observations[[group]] == response))))
  }
}

head(df)
```

If at this point you look to the R code, you will realize that the `Response` variable is identified as a number with a range from 0 to 4. Off course, we are completely aware that this response is not numeric, but and ordered (ordinal) categorical variable. So, we explicitly make the conversion:

```{r}
#| label: ConvertData
#| include: true
#| warning: false
#| message: false
#| column: margin

df$Response <- factor(df$Response, levels = c("0", "1", "2", "3", "4"), ordered = TRUE)

str(df)

```

Now, we can verify is recognized as an ordered categorical variable. With this in hand we easily proceed to visualization and modeling.

## Exploratory data visualization

Let's first load the required libraries and create a visual theme for our plots.

```{r}
#| label: LoadPack
#| include: true
#| warning: false
#| message: false

library(ggplot2)
library(brms)
library(ggdist)
library(easystats)
library(dplyr)
library(tibble)

Plot_theme <- theme_classic() +
  theme(
      plot.title = element_text(size=18, hjust = 0.5, face="bold"),
      plot.subtitle = element_text(size = 10, color = "black"),
      plot.caption = element_text(size = 12, color = "black"),
      axis.line = element_line(colour = "black", size = 1.5, linetype = "solid"),
      axis.ticks.length=unit(7,"pt"),
     
      axis.title.x = element_text(colour = "black", size = 16),
      axis.text.x = element_text(colour = "black", size = 16, angle = 0, hjust = 0.5),
      axis.ticks.x = element_line(colour = "black", size = 1),
      
      axis.title.y = element_text(colour = "black", size = 16),
      axis.text.y = element_text(colour = "black", size = 16),
      axis.ticks.y = element_line(colour = "black", size = 1),
      
      legend.position="right",
      legend.direction="vertical",
      legend.title = element_text(colour="black", face="bold", size=12),
      legend.text = element_text(colour="black", size=10),
      
      plot.margin = margin(t = 10,  # Top margin
                             r = 2,  # Right margin
                             b = 10,  # Bottom margin
                             l = 10) # Left margin
      ) 
```

One straight forward manner to visualize categorical data is by means of a bar plot.

```{r}
#| label: fig-Fig2
#| include: true
#| warning: false
#| message: false
#| fig-cap: Response colored by group.
#| fig-height: 5
#| fig-width: 6

ggplot(df, aes(x = factor(Response), fill = Group)) +
  geom_bar(position = "dodge") +
  labs(x = "Response", y = "Count", title = "Response by Group") +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1") +
  Plot_theme +
theme(legend.position = "top", legend.direction = "horizontal")
```

@fig-Fig2 shows the frequency per group. This is more in line with the logic of a categorical variable, rather than box plots which are more relevant for continuous numerical variables. Now, let's run a regression to disentangle the mysteries (if any) of this dataset.

# Fitting an ordinal regression with brms

We'll use `brms` to fit a `cumulative` model. This model assumes that the neurological score originates Y from the categorization of a (probably) latent (but not observable or measured) continuous variable Y˜ [@bürkner2019]. As usual in most `brms` tutorials, I need to apologize for skipping the prior issue. Let's assume an "I have no idea" mentality and let the default (flat) `brms` priors do the dirty job.

We fit the `cumulative` model following the usual formula syntax and adding `cumulative("probit")` as a family (assuming the latent variable and corresponding error term is normally distributed.We have only one predictor variable, the experimental group each animal belongs to.

```{r}
#| label: OrdinalFit
#| include: true
#| warning: false
#| message: false
#| results: false

Ordinal_Fit <- brm(Response ~ Group, 
           data = df, 
           family = cumulative("probit"),
           # seed for reproducibility purposes
           seed = 8807,
           control = list(adapt_delta = 0.99),
           # this is to save the model in my laptop
           file    = "Models/2024-04-03_UseAndAbuseANOVA/Ordinal_Fit.rds",
           file_refit = "never")

# Add loo for model comparison
Ordinal_Fit <- 
  add_criterion(Ordinal_Fit, c("loo", "waic", "bayes_R2"))
```

Before looking at the results, let's perform a model diagnostics to compare the observation and the model predictions.

## Model diagnostics

We plot the model diagnostics as done before:

```{r}
#| label: fig-OrdinalDiag1
#| include: true
#| warning: false
#| message: false
#| fig-cap: Model diagnostics for the ordinal regression
#| fig-height: 4
#| fig-width: 5

set.seed(8807)

pp_check(Ordinal_Fit, ndraws = 100) +
  labs(title = "Ordinal regression") +
  theme_classic()
```

From @fig-OrdinalDiag1, I speculate that the uneven distribution (variance) of scores across the groups may cause such deviations. Later, we'll see if the prediction of the variance produces better estimates. For the moment, we are good to go.

## Checking the results for the ordinal regression

Let's take a look at the posterior distribution using the `describe_posterior` function from the `bayestestR` package [@bayestestR], as an alternative to the typical `summary` function.

```{r}
#| label: Student_Posterior
#| include: true
#| warning: false
#| message: false

describe_posterior(Ordinal_Fit,
                   centrality = "mean",
                   dispersion = TRUE,
                   ci_method = "HDI",
                   test = "rope",
                   )
```

The thresholds (score thresholds) are labeled as "Intercepts" in this model, which are applicable for our base condition "sham". The coefficients for GrouptMCAO and GrouptMCAO_C indicate the difference from sham animals on the latent Y˜ scale. Therefore, we see that GrouptMCAO presents 8.6 standard deviation higher scores on the latent Y˜ scale.

I want to refer here to a crucial (and not trivial) aspect as a reminder for a future post (stay tuned). I judge that it is irrelevant to make comparisons between a group that does not have a distribution (being 0 in all the cases) like the sham group, with a group that does have a distribution (the experimental groups). If you think about it carefully, the purpose of this procedure is null. Let us, however, follow the thread of modern scientific culture and not think too much about it.

Of course, appreciating the difference between the groups becomes more feasible if we look at it visually with the`conditional effects` function from `brms`.

```{r}
#| label: fig-OrdinalEff
#| include: true
#| warning: false
#| message: false
#| fig-cap: Conditional effects for the Ordinal model
#| fig-height: 5
#| fig-width: 7


Ordinal_CondEffects <- 
  conditional_effects(Ordinal_Fit, "Group", categorical = TRUE)

Ordinal_CondEffects <- plot(Ordinal_CondEffects, 
       plot = FALSE)[[1]]

Ordinal_CondEffects + 
  Plot_theme +
  theme(legend.position = "bottom", legend.direction = "horizontal")
```

Strangely (or perhaps not so much from the MCMC simulation framework), the model estimates that the sham group can have values of 1, although we have not seen it in the data. The reason is, no doubt, that the model estimates a common variance for all groups. We will see if this changes when our model also includes the variance as response.

Otherwise, the estimates we obtain for the tMCAO and tMCO_C groups are much more in line with the data. Thus, we can make more precise statements, instead of running an ANOVA (incorrectly for ordinal variables) and saying that there is "a significant difference" between one group and the other, which is the same as saying nothing. For example, the model tells us that scores of 2-4 for the tMCAO group have a very similar probability (about 25%). The case is different for the tMCAO_C group whose probability of 2 in the neurological score is higher (with considerable uncertainty) than for the other scores. If I were faced with this dataset and this model, I would assert that the probability that the tMCAO_C group reflects less neurological damage (based on scores 1 and 2) is higher than for the tMCAO group.

Can we obtain precise numbers that quantify the differences between the scores of different groups and their uncertainty? Of course we can! using the `emmeans` package [@emmeans]. However, that will be topic for another post (stay tuned).

# Including the variance as a response variable

For this sort of cumulative models there is no `sigma` parameter. Instead, to account for unequal variances we must employ a parameter called `disc`. For more on this aspect, please refer to the tutorial of Ordinal regression by Paul-Christian Bürkner, the creator of `brms` [@bürkner2019].

```{r}
#| label: OrdinalFit_Sigma
#| include: true
#| warning: false
#| message: false
#| results: false

Ordinal_Mdl2 <- bf (Response ~ Group) +
                     lf(disc ~  0 + Group, cmc = FALSE)

Ordinal_Fit2 <- brm( 
           formula = Ordinal_Mdl2,
           data = df, 
           family = cumulative("probit"),
           # seed for reproducibility purposes
           seed = 8807,
           control = list(adapt_delta = 0.99),
           # this is to save the model in my laptop
           file    = "Models/2024-04-03_UseAndAbuseANOVA/Ordinal_Fit2.rds",
           file_refit = "never")

# Add loo for model comparison
Ordinal_Fit2 <- 
  add_criterion(Ordinal_Fit2, c("loo", "waic", "bayes_R2"))
```

## Model diagnostics

```{r}
#| label: fig-OrdinalDiag2
#| include: true
#| warning: false
#| message: false
#| fig-cap: Model diagnostics for our model predicting the variance
#| fig-height: 4
#| fig-width: 5

set.seed(8807)

pp_check(Ordinal_Fit2, ndraws = 100) +
  labs(title = "Student-t") +
  theme_classic()
```

@fig-OrdinalDiag2 Indeed, my expectations were not fulfilled. Including the variance as a response in the model does not represent a better alignment of the predictions to the data. The trend is maintained but the predictions markedly vary. Still, this is a generative model candidate.

## Checking the results for our new model

We visualize the posterior distribution as done before:.

```{r}
#| label: Ordinal_Posterior2
#| include: true
#| warning: false
#| message: false

describe_posterior(Ordinal_Fit2,
                   centrality = "mean",
                   dispersion = TRUE,
                   ci_method = "HDI",
                   test = "rope",
                   )
```

We can see a marked difference in the coefficients compared to the first model. The coefficient for "GrouptMCAO" increases form 8.6 to 15.9 and the one for "GrouptMCAO_C" upraises from 7 to 10.8. Undoubtedly, this model presents us with a different picture. Otherwise, the variance term is depicted under the names of "disc_GrouptMCAO" and "disc_GrouptMCAO_C". We see that both variances strongly differ form our base value "sham".

Now, let's plot the results:

```{r}
#| label: fig-OrdinalCondEffects2
#| include: true
#| warning: false
#| message: false
#| fig-cap: Conditional effects for our model including the variance as a respinse
#| fig-height: 5
#| fig-width: 5
#| layout-nrow: 1

Ordinal_CondEffects2 <- 
  conditional_effects(Ordinal_Fit2, categorical = TRUE)

Ordinal_CondEffects2 <- plot(Ordinal_CondEffects2, 
       plot = FALSE)[[1]]

Ordinal_CondEffects2 +
  Plot_theme +
  theme(legend.position = "bottom", legend.direction = "horizontal")

```

Contrary to what I expected, the model still predicts that sham animals have a little probability of obtaining a score 1. What we confirmed here is that this prediction is not based on assuming (falsely) that all groups have the same variance. Nonetheless, is still a logical prediction (not irracional) under this framework (ordinal regression), which is based on thresholds. If we go to Richard McElreath's Statistical Rethinking [@mcelreath2020k] we find the same situation with monkeys pulling levers. Fitting a more restricted model will imply using informative priors. We leave that for a future post. I know, I have made three promises here but I will fulfill them.

In this model, the probabilities for the different scores in the tMCAO group shift slightly. Still, given the high uncertainty, I will not change my conclusion on the performance of this group based on this model. For the tMCAO_C group, on the other hand, the predictions do not shift in a way easily detectable to the eye. Let's finish this blog post by comparing the two models.

## Model comparison

We perform model comparison using the the `loo` package [@loo; @vehtari2016] for leave-one-out cross validation. For an alternative approach using the WAIC criteria [@gelman2013] I recommend you to check [this](https://medium.com/towards-data-science/do-not-over-think-about-outliers-use-a-student-t-distribution-instead-b6c584b91d5c) post also publish by Towards Data Science.

```{r}
#| label: Models_Compare
#| include: true
#| warning: false
#| message: false
#| results: false

loo(Ordinal_Fit, Ordinal_Fit2)
```

Under this scheme, the models have a very similar performance. Indeed, the first model is slightly better for out of sample predictions. Accounting for the variance did not help much in this specific case, where perhaps relying in informative priors can unlock the next step of scientific inference.

I appreciate if you leave your comments or feedback letting me know if this journey was useful to you. If you want more quality content on data science and other topics, you may consider becoming a [medium member](https://medium.com/membership).

In the future, you may find an updated version of this post on my [GitHub site](https://github.com/daniel-manrique/MediumBlog/blob/main/2024-04-03_UseAndAbuseANOVA.qmd). 

# References

::: {#refs}
:::

```{r}
sessionInfo()
```
